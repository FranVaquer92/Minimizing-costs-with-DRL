{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CASO PRÁCTICO 2 - MINIMIZACIÓN DE COSTES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries and another python files\n",
    "import os\n",
    "import numpy as np\n",
    "import random as rn\n",
    "\n",
    "import environment\n",
    "import brain\n",
    "import dqn\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='TRUE'\n",
    "\n",
    "#Set up reproducibility seeds\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "np.random.seed(42)\n",
    "rn.seed(12345)\n",
    "\n",
    "#SET UP OF THE PARAMETERS\n",
    "epsilon = 0.3\n",
    "number_actions = 5\n",
    "direction_boundary = (number_actions - 1)/2\n",
    "number_epochs = 100\n",
    "max_memory = 3000\n",
    "batch_size = 512\n",
    "temperature_step = 1.5\n",
    "\n",
    "#BUILD OF THE ENVIRONMENT BY THE CREATION OF ENVIRONMENT OBJECT\n",
    "env = environment.Environment(optimal_temperature = (18.0, 24.0), initial_month = 0, initial_number_users = 20, initial_rate_data = 30)\n",
    "\n",
    "#BUILD OF THE BRAIN BY THE CRATION OF BRAIN OBJECT\n",
    "brain = brain.Brain(learning_rate = 0.00001, number_actions = number_actions)\n",
    "\n",
    "#BUILD OF THE DQN MODEL BY THE CREATION OF DQN OBJECT\n",
    "dqn = dqn.DQN(max_memory = max_memory, discount_factor = 0.9)\n",
    "\n",
    "#ELECTION OF TRAIN MODE\n",
    "train = True\n",
    "\n",
    "#TRAINING THE AI\n",
    "env.train = train\n",
    "model = brain.model\n",
    "\n",
    "early_stopping = True\n",
    "patience = 10\n",
    "best_total_reward = -np.inf\n",
    "patience_count = 0\n",
    "\n",
    "if env.train:\n",
    "    for epoch in range(1, number_epochs):\n",
    "        total_reward = 0\n",
    "        loss = 0.\n",
    "        new_month = np.random.randint(0,12)\n",
    "        env.reset(new_month = new_month)\n",
    "        game_over = False\n",
    "        current_state, _, _ = env.observe()\n",
    "        timestep = 0\n",
    "        #INITIALIZATION OF TIMESTEP BUCLE (Timestep = 1 minute) PER EPOCH\n",
    "        while ((not game_over) and (timestep <= (5*30*24*60))):\n",
    "            #RUN THE NEXT EXPLORATION ACTION\n",
    "            if np.random.rand() <= epsilon:\n",
    "                action = np.random.randint(0, number_actions)\n",
    "            #RUN THE NEXT INFERENCE ACTION\n",
    "            else:\n",
    "                q_values = model.predict(current_state)\n",
    "                action = np.argmax(q_values[0])\n",
    "            if (action < direction_boundary):\n",
    "                direction = -1\n",
    "            else:\n",
    "                direction = 1\n",
    "            energy_ai = abs(action - direction_boundary) * temperature_step\n",
    "            #UPLOAD ENVIRONMENT TO GET THE NEXT STATE\n",
    "            next_state, reward, game_over = env.update_env(direction, energy_ai, month= int(timestep/(30*24*60)))\n",
    "            total_reward += reward\n",
    "            \n",
    "            #STORE THE NEXT TRANSITION IN THE MEMORY\n",
    "            dqn.remember([current_state, action, reward, next_state], game_over)\n",
    "            \n",
    "            #GET THE TWO BLOCKS DIVIDE BY ENTRIES AND OBJECTIVES\n",
    "            inputs, targets = dqn.get_batch(model, batch_size)\n",
    "            \n",
    "            #CALCULATE THE LOSS FUNCTION USING ALL OF THE BLOCK OF ENTRIES AND OBJECTIVES\n",
    "            loss += model.train_on_batch(inputs, targets)\n",
    "            timestep += 1\n",
    "            current_state = next_state\n",
    "            \n",
    "        #PRINT TRINING RESULTS AT THE FINISH OF EPOCHS\n",
    "        print('\\n')\n",
    "        print('Epoch: {:03d}/{:03d}'.format(epoch, number_epochs))\n",
    "        print(' - TOTAL ENERGY WASTED BY THE SYSTEM WITHOUT AI: {:.0f} J'.format(env.total_energy_noai))\n",
    "        print(' - TOTAL ENERGY WASTED BY THE SYSTEM WITH AI: {:.0f} J'.format(env.total_energy_ai))\n",
    "        \n",
    "        #EARLY STOPPING\n",
    "        if early_stopping:\n",
    "            if total_reward <= best_total_reward:\n",
    "                patience_count += 1\n",
    "            else:\n",
    "                best_total_reward = total_reward\n",
    "                patience_count = 0\n",
    "            if patience_count >= patience:\n",
    "                print('Execution of early stopping')\n",
    "                break\n",
    "        \n",
    "        #SAVE THE MODEL TO USE IN FUTURE JOBS\n",
    "        model.save('model_dql_es1.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
